{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a general basic guide to pipeline from HuggingFace\n",
    "                                                                  - By Anirudh Gupta\n",
    "For more referrencing and clarity, refer the link : https://www.youtube.com/watch?v=QEaBAZQCtwE.\n",
    "To see more pipelines check out the link: https://huggingface.co/docs/transformers/main_classes/pipelines.\n",
    "To see different models, pipelines, etc. visit HuggingFace website. This has been taken from website as well as YouTube Videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998795986175537}, {'label': 'NEGATIVE', 'score': 0.9994852542877197}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline #must include this\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")   #Here sentiment-analysis is the model name, pipeline(...) does all your work\n",
    "result =classifier([\"I am very happy\", \"I am very sad\"])\n",
    "\n",
    "print(result)\n",
    "#Notes: \n",
    "#result[0]['score'] is the confidence of the model, more the confident the more likely it is\n",
    "#result[0]['label'] is the label, For various models it can change\n",
    "#in case we do not specify model, it uses defualt one, here it is distilbert-base-uncased-fine-tuned-sst-2-english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Today i ate a snack on a plate and I never said how excited I was to get the dish'}, {'generated_text': 'Today i ate one morning from two old students. i can understand when i first heard of this and'}]\n"
     ]
    }
   ],
   "source": [
    "#Now we will use a model and add more parameters, for what we want\n",
    "generator =pipeline (\"text-generation\", model=\"distilgpt2\")   ## generator, classifier are just fancy names\n",
    "result = generator(\n",
    "    \"Today i ate\", max_length=20, num_return_sequences=2\n",
    ")\n",
    "print(result)\n",
    "#Notes:\n",
    "#result[0]['generated_text'] is the generated text\n",
    "#num_return_sequences is the number of generated texts\n",
    "#max_length is the max length of the generated text\n",
    "# your computer downloads the model just a little bit and shoots you the ans, it will tell you the folder it stores in too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Underlying idea of pipeline\n",
    "Here we Use models and Tokenizer. If we want to finetune our model, we must understand this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998795986175537}, {'label': 'NEGATIVE', 'score': 0.9994852542877197}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model= AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "res= classifier([\"I am very happy\", \"I am very sad\"])\n",
    "print(res)\n",
    "#Notes:\n",
    "#tokenizer puts the text in Mathematical Representation which model evaluates over\n",
    "#AutoTokenizer.from_pretrained(model_name) just tokenizes the input for the model which is pre-trained\n",
    "#AutoModelForSequenceClassification.from_pretrained(model_name) loads the model which is pre-trained\n",
    "# just use .from_pretrained(model_name)\n",
    "\n",
    "# Notice we have the same result as before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Tokenizer more\n",
    "Tokenizer converts inputs to mathematical representation which model evaluates over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4824, 19204, 17629, 1999, 3733, 2126, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['understanding', 'token', '##izer', 'in', 'easy', 'way']\n",
      "[4824, 19204, 17629, 1999, 3733, 2126]\n",
      "understanding tokenizer in easy way\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model= AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sequence=\"Understanding Tokenizer in easy way\"\n",
    "\n",
    "res=tokenizer(sequence)\n",
    "print(res)\n",
    "# converts each token into its numerical representation and 101 and 102 means start and ending of lines\n",
    "# in attention mask, 1 means it is active and 0 means it is not active(not used in the model)\n",
    "tokens= tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids= tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string=tokenizer.decode(ids)\n",
    "print(decoded_string)\n",
    "#detailed explains how res is made, how tokens and ids and decoded string are used \n",
    "# Notice the final print value doesnt have capitals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Save a tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "save_directory=\"saved\"   #anyname save_directory or path, etc\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tok=AutoTokenizer.from_pretrained(save_directory)\n",
    "mod=AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "# put this at the end of your code, optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998795986175537}, {'label': 'NEGATIVE', 'score': 0.9994852542877197}]\n",
      "{'input_ids': tensor([[ 101, 1045, 2572, 2200, 3407,  102],\n",
      "        [ 101, 1045, 2572, 2200, 6517,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3359,  4.6890],\n",
      "        [ 4.1598, -3.4115]]), hidden_states=None, attentions=None)\n",
      "tensor([[1.2036e-04, 9.9988e-01],\n",
      "        [9.9949e-01, 5.1472e-04]])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "x_train = [\"I am very happy\", \"I am very sad\"]\n",
    "result=classifier(x_train)\n",
    "print(result)\n",
    "# This is what we do usually\n",
    "#Lets see Pytorch is used\n",
    "\n",
    "batch= tokenizer(x_train, padding=True, truncation= True, max_length=512, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions= F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels=torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of how to use this to use LLM's from HuggingFace\n",
    "This is an AI, well trained AI, so it will be faster for us to use. Similar fashion we will be able to use other LLM's from Hugging face. \n",
    "Whenever we are using models, there is a term 3b,7b,etc. Make sure for out laptops, it is <=7billion(7b), else computer may run very very slow( exceptions are always there)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
